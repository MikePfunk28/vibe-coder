pocketflow>=0.0.1
pyyaml>=6.0
python-dotenv>=1.0.1
typing_extensions>=4.0.0
requests>=2.31.0

# Local LLM Integration
llama-cpp-python>=0.1.70    # Direct llama.cpp integration
ollama>=0.1.0               # Ollama Python client

# Optional: For ONNX runtime (if you want ONNX support)
onnxruntime>=1.16.0
onnxruntime-gpu>=1.16.0     # If you have GPU support

# Optional: For Transformers/HuggingFace local models
transformers>=4.35.0
torch>=2.0.0               # PyTorch for local model inference
accelerate>=0.24.0         # For faster model loading
